---
title: "Project 1 - Xiaowo Sun"
runtime: shiny
output:
  html_document: 
    toc: true
    toc_depth: 2
  html_notebook: default
---

The inauguration of the President of the United States is a ceremony to mark the commencement of a new four-year term of a president of the United States. This ceremony takes place for each new presidential term, even if the president is continuing in office for a second term.

Newly sworn-in presidents usually give a speech referred to as an inaugural address. As with many inaugural customs, this one was started by George Washington in 1789. 

Most presidents use their inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today.

This notebook will show you some findings in presidents' inaugural addresses (hope you find they are interesting), from the perspectives of political parties and single speeches.


#Pre-work
```{r}
packages.used=c("SnowballC", "ggplot2", "rvest", "tibble", "qdap", 
                "sentimentr", "gplots", "dplyr", "tm", "syuzhet", 
                "factoextra", "scales", "RColorBrewer", "RANN", "tm",
                "topicmodels","NLP","openNLP","magrittr","wordcloud",
                "tidytext","stringr","data.table","shiny","XML","RCurl")

# check packages that need to be installed.
packages.needed=setdiff(packages.used, 
                        intersect(installed.packages()[,1], 
                                  packages.used))
# install additional packages
if(length(packages.needed)>0){
  install.packages(packages.needed, dependencies = TRUE)
}

# load packages
library(SnowballC)
library(dplyr)
library(rvest)
library(tibble)
library(qdap)
library(sentimentr)
library(gplots)
library(ggplot2)
library(syuzhet)
library(factoextra)
library(scales)
library(RColorBrewer)
library(RANN)
library(tm)
library(topicmodels)
library(NLP)
library(openNLP)
library(magrittr)
library(wordcloud)
library(tidytext)
library(stringr)
library(shiny)
library(data.table)
library(XML)
library(RCurl)
```

This notebook was prepared with the following environmental settings.
```{r}
print(R.version)
```

# Part 1: From the Perspective of Political Parties -- Republican vs. Democratic

Throughout most of its history, politics of the United States have been dominated by political parties. Since the 1860s, these two main parties have been the Republican Party and the Democratic Party. Here, we will look into and talk about the presidents' inaugural speeches of these two parties.

##1.1 Load Speeches 
```{r}
##Republican corpus
Rpbl.txt <- file.path("../data/InauguralSpeeches/Republican")
Rpbl <- Corpus(DirSource(Rpbl.txt))
Rpbl
##Democratic corpus
Dmcr.txt <- file.path("../data/InauguralSpeeches/Democratic")
Dmcr <- Corpus(DirSource(Dmcr.txt))
Dmcr
```

##1.2 Words Frequency
###Overview -- Using Wordcloud
To gain an general sense of the words used, I first use the (`wordcloud`).
From wordcloud, we can see that, throughout the history, the two parties used similar words in inaugural speeches simply in terms of the high frequecies of several common words, which are *will*, *people*, *government*, *can*, *must*, etc. And there are only a few words that show a little bit difference. We shall explore further to see more about that.

#### -- Republican
```{r}
#pre-processing
Rpbl <- tm_map(Rpbl, removePunctuation)
Rpbl <- tm_map(Rpbl, removeNumbers)
Rpbl <- tm_map(Rpbl, tolower)
Rpbl <- tm_map(Rpbl, removeWords, stopwords("english"))
Rpbl <- tm_map(Rpbl, removeWords, c("upon"))
Rpbl <- tm_map(Rpbl, stemDocument)
Rpbl <- tm_map(Rpbl, stripWhitespace)
Rpbl <- tm_map(Rpbl, PlainTextDocument)

tdm.Rpbl <- TermDocumentMatrix(Rpbl)   
tdm.tidyRpbl=tidy(tdm.Rpbl)
tdm.overallRpbl=summarise(group_by(tdm.tidyRpbl, term), sum(count))

#generate the wordcloud
wordcloud(tdm.overallRpbl$term, tdm.overallRpbl$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Reds")) 
```

#### -- Democratic
```{r}
#pre-processing
Dmcr <- tm_map(Dmcr, removePunctuation)
Dmcr <- tm_map(Dmcr, removeNumbers)
Dmcr <- tm_map(Dmcr, tolower)
Dmcr <- tm_map(Dmcr, removeWords, stopwords("english"))
Dmcr <- tm_map(Dmcr, removeWords, c("upon"))
Dmcr <- tm_map(Dmcr, stemDocument)
Dmcr <- tm_map(Dmcr, stripWhitespace)
Dmcr <- tm_map(Dmcr, PlainTextDocument)

tdm.Dmcr <- TermDocumentMatrix(Dmcr)   
tdm.tidyDmcr=tidy(tdm.Dmcr)
tdm.overallDmcr=summarise(group_by(tdm.tidyDmcr, term), sum(count))

#generate the wordcloud
wordcloud(tdm.overallDmcr$term, tdm.overallDmcr$`sum(count)`, max.words=70, random.order=FALSE, random.color=FALSE,rot.per=0, colors=brewer.pal(5,"Blues")) 
```

###Frequency of Top 30 Words
#### -- Republican
```{r}
dtm.Rpbl <- DocumentTermMatrix(Rpbl) 
freq.Rpbl <- colSums(as.matrix(dtm.Rpbl))
length(freq.Rpbl)

#sort the terms based on frequency in a descending order
ord.Rpbl <- order(freq.Rpbl, decreasing = TRUE)
#view top 30
freq.Rpbl[head(ord.Rpbl, n=30L)]
```

```{r}

wf.Rpbl <- data.frame(word=names(freq.Rpbl), freq=freq.Rpbl)
p.Rpbl <- ggplot(subset(wf.Rpbl, freq>76), aes(reorder(word, -freq),freq))    
p.Rpbl <- p.Rpbl + geom_bar(stat="identity",fill = "red3")
p.Rpbl <- p.Rpbl + theme(axis.text.x=element_text(angle=45, hjust=1)) 
p.Rpbl <- p.Rpbl + xlab('Top 30 words') + ylab('Frequency')
p.Rpbl

```

#### -- Democratic
```{r}
dtm.Dmcr <- DocumentTermMatrix(Dmcr) 
freq.Dmcr <- colSums(as.matrix(dtm.Dmcr))
length(freq.Dmcr)

#sort the terms based on frequency in a descending order
ord.Dmcr <- order(freq.Dmcr, decreasing = TRUE)
#view top 30
freq.Dmcr[head(ord.Dmcr, n=30L)]
```

```{r}
wf.Dmcr <- data.frame(word=names(freq.Dmcr), freq=freq.Dmcr)
p.Dmcr <- ggplot(subset(wf.Dmcr, freq>55), aes(reorder(word, -freq),freq))    
p.Dmcr <- p.Dmcr + geom_bar(stat="identity",fill = "blue4")
p.Dmcr <- p.Dmcr + theme(axis.text.x=element_text(angle=45, hjust=1))
p.Dmcr <- p.Dmcr + xlab('Top 30 words') + ylab('Frequency')
p.Dmcr
```
Still, there is not much difference between the words used by two parties. To choose several words describing things that two parties identically pursue or provide, for the Republicans, the words would be *freedom*, *law*, *public*, and for the Democrats, the words would be *power*, *constitution*, *spirit*.

Considering the high frequency of *new*, *great*, *peace*, I irresponsibly guess that presidents were commiting themselves to bringing changes and something great to the nation, and safeguarding the world peace.

##Interactive Time!
####Customize the frequency ranges to find out more!
```{r, warning=FALSE, message=FALSE, echo=FALSE}

shinyApp(
  
    ui = fluidPage(
       titlePanel("Select Frequency Ranges for Parties"),
  
       fluidRow(style = "padding-bottom: 100px;",
            column(4, sliderInput('freq1', 'Frequency Range for Republican:',
                               min = 30, max = 200, value = c(50,100))),
            plotOutput('plotR', height = "400px")
       ),
        
       fluidRow(style = "padding-bottom: 100px;",
            column(4, sliderInput('freq2', 'Frequency Range for Democratic:',
                               min = 30, max = 200, value = c(40,70))),
            plotOutput('plotD', height = "400px")
      )
    ),

    server = function(input, output){
      
        output$plotR <- renderPlot({
          p.Rpbl <- ggplot(subset(wf.Rpbl, freq>min(input$freq1)&freq<max(input$freq1)), aes(reorder(word, -freq),freq))
          p.Rpbl <- p.Rpbl + geom_bar(stat="identity",fill = "red3")
          p.Rpbl <- p.Rpbl + theme(axis.text.x=element_text(angle=45, hjust=1)) 
          p.Rpbl <- p.Rpbl + xlab('Word') + ylab('Frequency')
          p.Rpbl
        })
        
        output$plotD <- renderPlot({
          p.Dmcr <- ggplot(subset(wf.Dmcr, freq>min(input$freq2)&freq<max(input$freq2)), aes(reorder(word, -freq),freq))
          p.Dmcr <- p.Dmcr + geom_bar(stat="identity",fill = "blue4")
          p.Dmcr <- p.Dmcr + theme(axis.text.x=element_text(angle=45, hjust=1))
          p.Dmcr <- p.Dmcr + xlab('Word') + ylab('Frequency')
          p.Dmcr
        })
      },

options = list(height = 1100)  
  
)

```


#Part 2: From the Perspective of Single Speeches
#### -- take Donald Trump as an example
This part, I mainly provide the visualization of the sentiment development, the readability and the memorability of every inaugural speech since 1789.

##2.1 Load Speeches
```{r, warning=FALSE, message=FALSE}
folder.pathA="../data/InauguralSpeeches/All"
speeches.A=list.files(path = folder.pathA, pattern = "*.txt")
ff.A<-Corpus(DirSource(folder.pathA))

```

##2.2 Sentiment Development
Here I apply qdap’s sentiment analysis to speeches. The qdap’s sentiment analysis is based on a sentence-level formula classifying each word as either positive, negative, neutral, negator or amplifier, per [Hu & Liu’s sentiment lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html). 

Through the variation, the trendline reveals sentiment along with the increasing positivity in general.
```{r, warning=FALSE, message=FALSE}

speech.df <- data.table(speech=ff.A[[9]]$content, person=ff.A[[9]]$meta$id)
sentences <- data.table(sentSplit(speech.df, "speech"))
sentences <- sentences[, sentence.num := seq(nrow(sentences))]
sentences <- sentences[, tot := NULL]
setcolorder(sentences, c("person", "sentence.num", "speech"))
#syllables per sentence
sentences <- sentences[, syllables := syllable_sum(speech)]
#add cumulative syllable count and percent complete as proxy for progression
sentences <- sentences[, syllables.cumsum := cumsum(syllables)]
sentences <- sentences[, pct.complete := syllables.cumsum / sum(sentences$syllables)]
sentences <- sentences[, pct.complete.100 := pct.complete * 100]
pol.df <- polarity(sentences$speech)$all
sentences <- sentences[, words := pol.df$wc]
sentences <- sentences[, pol := pol.df$polarity]

#customize and generate plot
my.theme <- 
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12),
        plot.title=element_text(size=16, hjust=0.5))

Plot.setting <- function(gg)
  return(gg + geom_point(color="salmon") + 
           stat_smooth(color="red3", fill="lightgray", size=1.4) + 
           xlab("Percent complete (by syllable count)") + 
           scale_x_continuous(labels = percent) + my.theme)

Plot.setting(ggplot(sentences, aes(pct.complete, pol)) +
                    ylab("Sentiment (sentence-level polarity)") + 
                    ggtitle(ff.A[[9]]$meta$id))

```


##2.3 Readability

Readability tests are typically based on syllables, words, and sentences in order to approximate the grade level required to comprehend a text. qdap offers several of the most popular formulas, of which I chose the [Automated Readability Index](https://en.wikipedia.org/wiki/Automated_readability_index). 

Here we can see that the readability is mostly constant throughout the speech, though it varies within each section. This makes sense, as one generally avoids too many simple or complex sentences in a row. The index is around 8 to 9, which refers to the grade level of eighth to ninth.
```{r,warning=FALSE, message=FALSE}
sentences <- sentences[, readability := scores(automated_readability_index(speech,sentence.num, output = "valid"))$Automated_Readability_Index]

Plot.setting(ggplot(sentences, aes(pct.complete, readability)) +
                    ylab("Automated Readability Index") +
                    ggtitle(ff.A[[9]]$meta$id))
```


##2.4 Memorability

Google search results can serve as a useful indicator of public opinion. I use a function from [theBioBucket’s blog post](http://thebiobucket.blogspot.com/2012/03/playing-with-xml-get-no-of-google.html) to count Google hits for a query. 
Alert: Google will block you after about the 300th recursion!
```{r,warning=FALSE, message=FALSE}
#scraping Google search hits

GoogleHits <- function(query){
  require(XML)
  require(RCurl)
  
  url <- paste0("https://www.google.com/search?q=", gsub(" ", "+", query))
  
  CAINFO = paste0(system.file(package="RCurl"), "/CurlSSL/ca-bundle.crt")
  script <- getURL(url, followlocation=T, cainfo=CAINFO)
  doc <- htmlParse(script)
  res <- xpathSApply(doc, '//*/div[@id="resultStats"]', xmlValue)
  return(as.numeric(gsub("[^0-9]", "", res)))
}

```

A quick plot reveals that there’s a huge difference between the most-quoted sentences and the rest of the speech, particularly the top ten. Do these top sentences align with your expectations?
```{r,warning=FALSE, message=FALSE}
##sometimes this chunk may take long time to run or report an error, as the function GoogleHits is not stable due to external factors. If this happens then, please just re-open the Rstudio and try again later. 
#pass each sentence to the function, stripped of punctuation and grouped in brackets, and with “mlk” added to ensure it related to the speech
sentences <- sentences[, google.hits := GoogleHits(paste0("[", gsub("[,;!.]", "", speech), 
                                             "] mlk"))]
ggplot(sentences, aes(pct.complete, google.hits / 1e6)) +
  geom_line(color="red3") + # Lighten dots
  xlab("Percent complete (by syllable count)") + 
  scale_x_continuous(labels = percent) + my.theme +
  ylim(0, max(sentences$google.hits) / 1e6) +
  ylab("Sentence memorability (millions of Google hits)") +
  ggtitle("Memorability")
```


```{r,warning=FALSE, message=FALSE}

head(sentences[order(-google.hits)]$speech, 10)
```

```{r,warning=FALSE, message=FALSE}
#plotting Google hits on a log scale reduces skew and allows us to work on a ratio scale
sentences <- sentences[, log.google.hits := log(google.hits)]

Plot.setting(ggplot(sentences, aes(pct.complete, log.google.hits)) +
                    ylab("Memorability (log of sentence's Google hits)") +
                    ggtitle(ff.A[[9]]$meta$id))
```

##Interactive time!
####Select the speech and to find out more!
####Questions: 
Which speech is the most memorable and which one is the most forgetable? 
Which speeches have more sentiment swings?
Which speech has the most readable content?
......
```{r, warning=FALSE, message=FALSE, echo=FALSE}

shinyApp(
  
   ui = fluidPage(
      titlePanel("Select Speeches"),
      
      fluidRow(style = "padding-bottom: 20px;",
        column(4, selectInput('speech1', 'Inaugural Speech',
                              speeches.A,
                              selected=speeches.A[1]))
      ),
        
      fluidRow(style = "padding-bottom: 20px;",
        splitLayout(cellWidths = c("33%","33%","33%"), 
                    plotOutput('plot1s', height = "300px"), 
                    plotOutput('plot1r', height = "300px"),
                    plotOutput('plot1m', height = "300px"))
      ),
        
      fluidRow(style = "padding-bottom: 20px;",
        splitLayout(cellWidths = c("50%","50%"), 
                    plotOutput('plot1ms', height = "300px"),
                    tableOutput('sord')
                    )
      )
      
    ),

    server <- function(input, output){
      
      selectData1 <- reactive({
        
        speech.input1 <- ff.A[[input$speech1]]$content
        speech.df1 <- data.table(speech=speech.input1, person="DJN")
        sentences1 <- data.table(sentSplit(speech.df1, "speech"))
        sentences1[, sentence.num := seq(nrow(sentences1))]
        sentences1[, tot := NULL]
        setcolorder(sentences1, c("person", "sentence.num", "speech"))
        # Syllables per sentence
        sentences1[, syllables := syllable_sum(speech)]
        # Add cumulative syllable count and percent complete as proxy for progression
        sentences1[, syllables.cumsum := cumsum(syllables)]
        sentences1[, pct.complete := syllables.cumsum / sum(sentences1$syllables)]
        sentences1[, pct.complete.100 := pct.complete * 100]
        pol.df1 <- polarity(sentences1$speech)$all
        sentences1[, words := pol.df$wc]
        sentences1[, pol := pol.df$polarity]
        sentences1[, readability := scores(automated_readability_index(speech,sentence.num, output = "valid"))$Automated_Readability_Index]
        sentences1[, google.hits := GoogleHits(paste0("[", gsub("[,;!.]", "", speech), "] mlk"))]
        sentences1[, log.google.hits := log(google.hits)]
        
      })
      
      
      
        output$plot1s <- renderPlot({
          Plot1.setting <- function(gg)
  return(gg + geom_point(color="salmon") + 
           stat_smooth(color="red3", fill="lightgray", size=1.4) + 
           xlab("Percent complete (by syllable count)") + 
           scale_x_continuous(labels = percent) + my.theme)
        
        Plot1.setting(ggplot(selectData1(), aes(pct.complete, pol)) +
                    ylab("Sentiment (sentence-level polarity)") + 
                    ggtitle("Sentiment"))
        })
        
        output$plot1r <- renderPlot({
          Plot1.setting <- function(gg)
  return(gg + geom_point(color="salmon") + 
           stat_smooth(color="red3", fill="lightgray", size=1.4) + 
           xlab("Percent complete (by syllable count)") + 
           scale_x_continuous(labels = percent) + my.theme)
          
        Plot1.setting(ggplot(selectData1(), aes(pct.complete, readability)) +
                    ylab("Automated Readability Index") +
                    ggtitle("Readability"))
        })
        
        output$plot1m <- renderPlot({
        Plot1.setting <- function(gg)
  return(gg + geom_point(color="salmon") + 
           stat_smooth(color="red3", fill="lightgray", size=1.4) + 
           xlab("Percent complete (by syllable count)") + 
           scale_x_continuous(labels = percent) + my.theme)
        Plot1.setting(ggplot(selectData1(), aes(pct.complete, log.google.hits)) +
                    ylab("Memorability (log of sentence's Google hits)") +
                    ggtitle("Memorability"))
        })
            
          
        output$plot1ms <- renderPlot({
        
        ggplot(selectData1(), aes(pct.complete, google.hits / 1e6)) +
  geom_line(color="red3") + 
  xlab("Percent complete (by syllable count)") + 
  scale_x_continuous(labels = percent) + my.theme +
  ylim(0, max(sentences$google.hits) / 1e6) +
  ylab("Sentence memorability (millions of Google hits)") +
  ggtitle("Sentence memorability & Top 10 Sentences")
          
        })
        
        output$sord <- renderTable({
           head(selectData1()[order(-google.hits)]$speech, 10)
          
        })
        
    },

  option = list(height = 1000)
    
)
```

